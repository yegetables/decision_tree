[数据集链接](https://tianchi.aliyun.com/competition/entrance/531830/information)

使用 决策树分类算法默认参数未调优，准确率大概在在0.7以上，
可以通过调参，或者使用其他算法来提高准确率。


ID3算法 信息增益来选择最佳的特征进行节点划分，以构建决策树，对于具有离散特征的数据集表现良好，但对于连续特征的处理较为困难。

C45算法 信息增益比来选择最佳的特征进行节点划分，以构建决策树，能够处理连续特征，并且引入了剪枝策略，提高了模型的泛化能力。C4.5算法在特征多样性和数据集较大时表现较好。

CART算法既可以用于分类问题，也可以用于回归问题。
CART算法使用基尼不纯度（Gini Impurity），进行特征选择；对于回归问题，CART算法使用平方误差来进行特征选择
优势在于其灵活性，既可以处理分类问题，也可以处理回归问题。


基尼不纯度（Gini impurity）：
基尼不纯度是用来衡量分类问题中节点的不纯度的指标，计算方式为节点中每个类别的概率乘以其余类别的概率之和。基尼不纯度的取值范围为0到0.5，值越小表示节点的纯度越高。
优势：基尼不纯度在计算上相对简单，不需要计算对数，计算速度较快。
适用场景：基尼不纯度通常适用于分类问题，特别是在二分类问题中表现较好。

信息增益（Information gain）：
信息增益是用来衡量节点分裂前后信息纯度的变化，计算方式为父节点的信息熵减去子节点的加权平均信息熵。信息增益越大表示节点的纯度提升越高。
优势：信息增益可以在处理多分类问题时更加灵活，对于多分类问题的节点分裂效果可能更好。
适用场景：信息增益适用于分类问题，尤其是在多分类问题中表现较好。


| 贷款违约预测     |
| ---------------- | ----- | ----- | ----- |
| 数据\准确度/算法 | ID3   | CART  | C45   | 数据概率 |
|1k|0.675|0.65|0.72|0.765
|1k方差|0.325|0.35|0.28|
| 1w               | 0.7055 | 0.7035 |0.797 |0.7935   |
| 1w方差           | 0.2945 | 0.2965 | 0.2065 |
| 10w              | 0.7039 | 0.7029 | 0.801 | 0.8032    |
| 10w方差          | 0.2961 | 0.2971 | 0.19885 |
